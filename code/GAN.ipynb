{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'vgan'  # wgan or wgan-gp\n",
    "DATASET = '8gaussians'  # 8gaussians, 25gaussians, swissroll\n",
    "DIM = 512  # Model dimensionality\n",
    "#FIXED_GENERATOR = False  # whether to hold the generator fixed at real data plus\n",
    "# Gaussian noise, as in the plots in the paper\n",
    "LAMBDA = .1  # Smaller lambda seems to help for toy tasks specifically\n",
    "CRITIC_ITERS = 1  # How many critic iterations per generator iteration\n",
    "BATCH_SIZE = 256  # Batch size\n",
    "#ITERS = 100000  # how many generator iterations to train for\n",
    "MAX_EPOCH = 50\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            nn.Linear(2, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, 2),\n",
    "        )\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, noise, real_data):\n",
    "        output = self.main(noise)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            nn.Linear(2, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, 1),\n",
    "        )\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.main(inputs)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "frame_index = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(true_dist):\n",
    "    \"\"\"\n",
    "    Generates and saves a plot of the true distribution, the generator, and the\n",
    "    critic.\n",
    "    \"\"\"\n",
    "    N_POINTS = 128\n",
    "    RANGE = 3\n",
    "\n",
    "    points = np.zeros((N_POINTS, N_POINTS, 2), dtype='float32')\n",
    "    points[:, :, 0] = np.linspace(-RANGE, RANGE, N_POINTS)[:, None]\n",
    "    points[:, :, 1] = np.linspace(-RANGE, RANGE, N_POINTS)[None, :]\n",
    "    points = points.reshape((-1, 2))\n",
    "\n",
    "    points_v = autograd.Variable(torch.Tensor(points), volatile=True)\n",
    "    if use_cuda:\n",
    "        points_v = points_v.cuda()\n",
    "    disc_map = netD(points_v).cpu().data.numpy()\n",
    "\n",
    "    noise = torch.randn(BATCH_SIZE, 2)\n",
    "    if use_cuda:\n",
    "        noise = noise.cuda()\n",
    "    noisev = autograd.Variable(noise, volatile=True)\n",
    "    true_dist_v = autograd.Variable(torch.Tensor(true_dist).cuda() if use_cuda else torch.Tensor(true_dist))\n",
    "    samples = netG(noisev, true_dist_v).cpu().data.numpy()\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    x = y = np.linspace(-RANGE, RANGE, N_POINTS)\n",
    "    plt.contour(x, y, disc_map.reshape((len(x), len(y))).transpose())\n",
    "\n",
    "    plt.scatter(true_dist[:, 0], true_dist[:, 1], c='orange', marker='+')\n",
    "    if not FIXED_GENERATOR:\n",
    "        plt.scatter(samples[:, 0], samples[:, 1], c='green', marker='+')\n",
    "    \n",
    "#     plt.savefig('tmp/' + DATASET + '/' + 'frame' + str(frame_index[0]) + '.jpg')\n",
    "\n",
    "    frame_index[0] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset iterator\n",
    "def inf_train_gen():\n",
    "    if DATASET == '25gaussians':\n",
    "\n",
    "        dataset = []\n",
    "        for i in range(100000 / 25):\n",
    "            for x in range(-2, 3):\n",
    "                for y in range(-2, 3):\n",
    "                    point = np.random.randn(2) * 0.05\n",
    "                    point[0] += 2 * x\n",
    "                    point[1] += 2 * y\n",
    "                    dataset.append(point)\n",
    "        dataset = np.array(dataset, dtype='float32')\n",
    "        np.random.shuffle(dataset)\n",
    "        dataset /= 2.828  # stdev\n",
    "        while True:\n",
    "            for i in range(len(dataset) / BATCH_SIZE):\n",
    "                yield dataset[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "\n",
    "    elif DATASET == 'swissroll':\n",
    "\n",
    "        while True:\n",
    "            data = sklearn.datasets.make_swiss_roll(\n",
    "                n_samples=BATCH_SIZE,\n",
    "                noise=0.25\n",
    "            )[0]\n",
    "            data = data.astype('float32')[:, [0, 2]]\n",
    "            data /= 7.5  # stdev plus a little\n",
    "            yield data\n",
    "\n",
    "    elif DATASET == '8gaussians':\n",
    "\n",
    "        scale = 2.\n",
    "        centers = [\n",
    "            (1, 0),\n",
    "            (-1, 0),\n",
    "            (0, 1),\n",
    "            (0, -1),\n",
    "            (1. / np.sqrt(2), 1. / np.sqrt(2)),\n",
    "            (1. / np.sqrt(2), -1. / np.sqrt(2)),\n",
    "            (-1. / np.sqrt(2), 1. / np.sqrt(2)),\n",
    "            (-1. / np.sqrt(2), -1. / np.sqrt(2))\n",
    "        ]\n",
    "        centers = [(scale * x, scale * y) for x, y in centers]\n",
    "        while True:\n",
    "            dataset = []\n",
    "            for i in range(BATCH_SIZE):\n",
    "                point = np.random.randn(2) * .02\n",
    "                center = random.choice(centers)\n",
    "                point[0] += center[0]\n",
    "                point[1] += center[1]\n",
    "                dataset.append(point)\n",
    "            dataset = np.array(dataset, dtype='float32')\n",
    "            dataset /= 1.414  # stdev\n",
    "            yield dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "G_opt = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "batch_size = 64\n",
    "max_epoch = 50 # need more than 10 epochs for training generator\n",
    "step = 0\n",
    "#n_critic = 1 # for training more k steps about Discriminator\n",
    "\n",
    "D_labels = torch.ones(batch_size, 1).to(DEVICE) # Discriminator Label to real\n",
    "D_fakes = torch.zeros(batch_size, 1).to(DEVICE) # Discriminator Label to fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "netD.apply(weights_init)\n",
    "netG.apply(weights_init)\n",
    "print (netG)\n",
    "print (netD)\n",
    "\n",
    "if use_cuda:\n",
    "    netD = netD.cuda()\n",
    "    netG = netG.cuda()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * 0\n",
    "if use_cuda:\n",
    "    one = one.cuda()\n",
    "    mone = mone.cuda()\n",
    "\n",
    "data = inf_train_gen()\n",
    "\n",
    "    \n",
    "Disc_loss = np.array([])\n",
    "Gen_loss  = np.array([])\n",
    "\n",
    "\n",
    "for iteration in range(ITERS):\n",
    "    ############################\n",
    "    # (1) Update D network\n",
    "#     ###########################\n",
    "#     for p in netD.parameters():  # reset requires_grad\n",
    "#         p.requires_grad = True  # they are set to False below in netG update\n",
    "\n",
    "    for iter_d in range(CRITIC_ITERS):\n",
    "        _data = next(data)\n",
    "        real_data = torch.Tensor(_data)\n",
    "        if use_cuda:\n",
    "            real_data = real_data.cuda()\n",
    "        real_data_v = autograd.Variable(real_data)\n",
    "\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # train with real\n",
    "        D_real = netD(real_data_v)\n",
    "        D_real_loss = criterion(D_label)\n",
    "        \n",
    "        #D_real = D_real.mean()\n",
    "        #D_real.backward(mone)\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(BATCH_SIZE, 2)\n",
    "        if use_cuda:\n",
    "            noise = noise.cuda()\n",
    "        noisev = autograd.Variable(noise, volatile=True)  # totally freeze netG\n",
    "        fake = autograd.Variable(netG(noisev, real_data_v).data)\n",
    "        inputv = fake\n",
    "        D_fake = netD(inputv)\n",
    "        D_fake = D_fake.mean()\n",
    "        D_fake.backward(one)\n",
    "\n",
    "#         # train with gradient penalty\n",
    "#         gradient_penalty = calc_gradient_penalty(netD, real_data_v.data, fake.data)\n",
    "#         gradient_penalty.backward()\n",
    "\n",
    "        D_cost = D_fake - D_real + gradient_penalty\n",
    "        Wasserstein_D = D_real - D_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "    if not FIXED_GENERATOR:\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False  # to avoid computation\n",
    "        netG.zero_grad()\n",
    "\n",
    "        _data = next(data)\n",
    "        real_data = torch.Tensor(_data)\n",
    "        if use_cuda:\n",
    "            real_data = real_data.cuda()\n",
    "        real_data_v = autograd.Variable(real_data)\n",
    "\n",
    "        noise = torch.randn(BATCH_SIZE, 2)\n",
    "        if use_cuda:\n",
    "            noise = noise.cuda()\n",
    "        noisev = autograd.Variable(noise)\n",
    "        fake = netG(noisev, real_data_v)\n",
    "        G = netD(fake)\n",
    "        G = G.mean()\n",
    "        G.backward(mone)\n",
    "    \n",
    "        G_cost = -G\n",
    "        optimizerG.step()\n",
    "        \n",
    "    Disc_loss = np.append(Disc_loss, D_cost.cpu().data.numpy())\n",
    "    Gen_loss  = np.append(Gen_loss, G_cost.cpu().data.numpy())\n",
    "    if iteration % 100 == 99:\n",
    "        generate_image(_data)\n",
    "        print (D_cost)    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avinash",
   "language": "python",
   "name": "avinash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
